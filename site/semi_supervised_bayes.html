<html>
<head>
	<title>semi supervised naive bayes</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<link rel=StyleSheet href="style.css" type="text/css">
</head>

<body>

<p>
<a href="intro.html">&lt;&lt;&nbsp;&nbsp;semi supervised algorithms</a>&nbsp;&nbsp;&nbsp;
<a href="index.html">index</a>&nbsp;&nbsp;&nbsp;
<a href="does_it_do_any_better.html">does it do any better?&nbsp;&nbsp;&gt;&gt;</a>
</p>

<h1>semi supervised naive bayes</h1>

<h2>example using normal naive bayes</h2>

<p>
let's pretend we want to classify the new rss articles we receive each day to decide if we want to bother reading them.
in a normal naive bayes system, which i've discussed in <a href="rss.feed/p3/">a previous article</a>, we'd train a classifier with some pre collected articles that we have taken the time to label as either to-read or to-ignore.</br>
</p>

<p>
for example; say we have collected a huge historical corpus of six articles (taken from <a href="http://www.theregister.co.uk/">the register</a>
and <a href="http://perezhilton.com/">perez hilton</a>). to make full use of the data we would need to label all six examples, but
due to time constraints we have only had time to do four <small>(realistically though the number of unlabelled examples left over would be much
greater than the number of labelled examples)</small></br>
<table>
<thead>
<tr><td>feed</td>         <td>doc</td> <td>article text</td>                         <td>class</td></tr>
</thead>
<tbody>
<tr><td>the register</td> <td>1</td> <td>new linux goes fast</td>           <td>read</td></tr>
<tr><td>perez hilton</td> <td>2</td> <td>hollywood celebrity goes fast</td> <td>ignore</td></tr>
<tr><td>the register</td> <td>3</td> <td>quake runs on linux</td>           <td>read</td></tr>
<tr><td>perez hilton</td> <td>4</td> <td>quake occurs in hollywood</td>     <td>ignore</td></tr>
<tr><td>the register</td> <td>5</td> <td>linux website</td>                 <td>?</td></tr>
<tr><td>perez hilton</td> <td>6</td> <td>hollywood fashion website</td>     <td>?</td></tr>
</tbody>
</table>
</p>

<p>
a new article then comes along...
<table>
<thead><tr><td>feed</td>         <td>article text</td>            <td>class</td></tr></thead>
<tbody><tr><td>perez hilton</td> <td>movie on fashion</td>      <td>??</td></tr></tbody>
</table>
</p>

<p>
to which class should we assign this article?
</p>

<p>
in a traditional naive bayes we can train only using the four labelled examples and have no way of making use of the two unlabelled documents.</br>
</p>

<p>
for example; to classify for the above example of 'movie on fashion' we first build a term occurence matrix for the four training examples...
</p>

<p>
<table>
<thead>
<tr><td>class</td><td>doc#</td><td>new</td><td>linux</td><td>goes</td><td>fast</td><td>hollywood</td><td>celebrity</td><td>quake</td><td>runs</td><td>on</td><td>occurs</td><td>in</td></tr>
</thead>
<tbody>
<tr><td>read</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>ignore</td><td>2</td><td></td><td></td><td>1</td><td>1</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>read</td>  <td>3</td><td></td>1<td></td><td></td><td></td><td></td><td></td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr>
<tr><td>ignore</td><td>4</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td>1</td><td>1</td></tr>
</tbody>
</table>
</p>

<p>
and then calculate the predicted probabilities for both <tt>P(read)</tt> and <tt>P(ignore)</tt>
<pre>
P(read | 'movie on fashion')
~= P('movie on fashion' | read) * P(read) 
~= P('movie' | read) * P('on' | read) * P('fashion' | read) * P(read)  # naive bayes assumption
~= 0 * 1/2 * 0/2 * P(read)
~= 1/5 * 2/5 * 1/5 * P(read)                                           # additive smoothing
~= 2/125 * P(read)
~= 2/125 * 1/2
~= 2/250

P(ignore | 'movie on fashion')
~= P('movie on fashion' | ignore) * P(ignore) 
~= P('movie' | ignore) * P('on' | ignore) * P('fashion' | ignore) * P(ignore)  # naive bayes assumption
~= 0/2 * 0/2 * 0/2 * P(ignore)     
~= 1/5 * 1/5 * 1/5 * P(ignore)                                                 # additive smoothing
~= 1/125 * P(ignore)
~= 1/125 * 1/2
~= 1/250
</pre>
</p>

<p>
as the estimated <tt>P(read)</tt> > <tt>P(ignore)</tt> we would class this as an article to-read.
</p>

<p>
diving into this a little bit we can see this prediction is made entirely based on the presence of the term 'on' in a previous to-read article. 
the terms 'movie' and 'fashion', having never been seen in the training set, were not part of the decision...
</p>

<h2>example using semi supervised naive bayes</h2>

<p>
so how does a semi supervised version of naive bayes make use of the unlabelled documents?</br>
</p>

<p>
let's look at the same corpus as before, with four labelled and two unlabelled examples.
this time though we'll record not the class directly but the probability distribution instead...
<table>
<thead><tr><td>feed</td>         <td>doc</td> <td>article text</td>                <td>P(read)</td> <td>P(ignore)</td></tr></thead>
<tbody>
<tr><td>the register</td> <td>1</td> <td>new linux goes fast</td>           <td>1.00</td> <td>0.00</td></tr>
<tr><td>perez hilton</td> <td>2</td> <td>hollywood celebrity goes fast</td> <td>0.00</td> <td>1.00</td></tr>
<tr><td>the register</td> <td>3</td> <td>quake runs on linux</td>           <td>1.00</td> <td>0.00</td></tr>
<tr><td>perez hilton</td> <td>4</td> <td>quake occurs in hollywood</td>     <td>0.00</td> <td>1.00</td></tr>
<tr><td>the register</td> <td>5</td> <td>linux website</td>              <td>?</td><td>?</td></tr>
<tr><td>perez hilton</td> <td>6</td> <td>hollywood fashion website</td>  <td>?</td><td>?</td></tr>
</tbody>
</table>
</p>

<p>
first we classify documents 5 and 6 using labelled docs 1 thru 4.
</p>

<p>
<pre>
P(read | 'linux website')
~= P('linux website' | read)  *  P(read) 
~= P('linux' | read) * P('website' | read) * P(read)  # naive bayes assumption
~= 2/2 * 0/2 * P(read)
~= 3/4 * 1/4 * P(read)                                # additive smoothing
~= 3/16 * P(read)
~= 3/16 * 1/2
~= 3/32

P(ignore | 'linux website')
~= P('linux website' | ignore) * P(ignore) 
~= P('linux' | ignore) * P('website' | ignore)  * P(ignore)  # naive bayes assumption
~= 0/2 * 0/2 * P(ignore)     
~= 1/4 * 1/4 * P(ignore)                                     # additive smoothing
~= 1/16 * P(ignore)
~= 1/16 * 1/2
~= 1/32
</pre>
</p>

<p>
normalising these estimates gives...
<pre>
P(read | 'linux website)    ~= 3/32 ~= 0.75
P(ignore | 'linux website') ~= 1/32 ~= 0.25
</pre>
usually we would classify this as an article to read since 75% > 25% but
for the semi supervised case we want to keep this distribution.
</p>

<p>
similarily for 'hollywood fashion website'...
<pre>
P(read | 'hollywood fashion website')
~= P('hollywood' | read) * P('fashion' | read) * P('website' | read) * P(read) 
~= 0/2 * 0/2 * 0/2 * P(read) 
~= 1/5 * 1/5 * 1/5 * P(read) 
~= 1/125 * P(read)
~= 1/125 * 1/2
~= 1/250

P(ignore | 'hollywood fashion website')
~= P('hollywood' | ignore) * P('fashion' | ignore) * P('website' | ignore) * P(ignore)  
~= 2/2 * 0/2 * 0/2 * P(ignore) 
~= 3/5 * 1/5 * 1/5 * P(ignore) 
~= 3/125 * P(ignore)
~= 3/125 * 1/2
~= 3/250
</pre>
</p>

<p>
normalising these estimates gives...
<pre>
P(read | 'hollywood fashion website')   ~= 1/250 ~= 0.25
P(ignore | 'hollywood fashion website') ~= 3/250 ~= 0.75
</pre>
</p>

<p>
we can now insert these predicted probabilities into our table
</p>

<p>
<table>
<thead><tr><td>feed</td>         <td>doc</td> <td>article text</td>                <td>P(read)</td> <td>P(ignore)</td></tr></thead>
<tbody>
<tr><td>the register</td> <td>1</td> <td>new linux goes fast</td>           <td>1.00</td> <td>0.00</td></tr>
<tr><td>perez hilton</td> <td>2</td> <td>hollywood celebrity goes fast</td> <td>0.00</td> <td>1.00</td></tr>
<tr><td>the register</td> <td>3</td> <td>quake runs on linux</td>           <td>1.00</td> <td>0.00</td></tr>
<tr><td>perez hilton</td> <td>4</td> <td>quake occurs in hollywood</td>     <td>0.00</td> <td>1.00</td></tr>
<tr><td>the register</td> <td>5</td> <td>linux website</td>              <td>0.75</td> <td>0.25</td></tr>
<tr><td>perez hilton</td> <td>6</td> <td>hollywood fashion website</td>  <td>0.25</td> <td>0.75</td></tr>
</tbody>
</table>
</p>

<p>
and given new values we can now <em>reclassify</em> documents 5 and 6 using a classifier trained with all 6</br>
<table>
<thead>
<tr>
	<td>read</td><td>ignore</td><td>doc#</td>
	<td>new</td><td>linux</td><td>goes</td>
	<td>fast</td><td>hollywood</td><td>celebrity</td>
	<td>quake</td><td>runs</td><td>on</td><td>occurs</td>
	<td>in</td><td>website</td><td>fashion</td>
</tr>
</thead>
<tbody>
<tr><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr>
<tr><td>0</td><td>1</td><td>2</td><td> </td><td> </td><td>1</td><td>1</td><td>1</td><td>1</td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr>
<tr><td>1</td><td>0</td><td>3</td><td> </td><td>1</td><td> </td><td> </td><td> </td><td> </td><td>1</td><td>1</td><td>1</td><td>1</td><td> </td><td> </td><td> </td></tr>
<tr><td>0</td><td>1</td><td>4</td><td> </td><td> </td><td> </td><td> </td><td>1</td><td> </td><td>1</td><td> </td><td> </td><td> </td><td>1</td><td> </td><td> </td></tr>
<tr><td>0.75</td><td>0.25</td><td>5</td><td> </td><td>1</td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>1</td><td> </td></tr>
<tr><td>0.25</td><td>0.75</td><td>6</td><td> </td><td> </td><td> </td><td> </td><td>1</td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>1</td><td>1</td></tr>
</tbody>
</table>
</p>

<p>
<pre>
P(read | 'linux website')
~= P('linux website' | read)  *  P(read) 
~= P('linux' | read) * P('website' | read) * P(read)
~= 2.75/3 * 1/3 * P(read)
~= 2.75/9 * P(read)
~= 2.75/9 * 3/6
~= 0.152

P(ignore | 'linux website')
~= P('linux website' | ignore) * P(ignore) 
~= P('linux' | ignore) * P('website' | ignore) * P(ignore)
~= 0.25/3 * 1/3 * P(ignore)     
~= 0.25/9 * P(ignore)                                                                
~= 0.25/9 * 3/6
~= 0.013 

P(read | 'website on linux')   ~= 0.152 ~= 0.92
P(ignore | 'website on linux') ~= 0.013 ~= 0.08
</pre>
</p>

<p>
<pre>
P(read | 'hollywood fashion website')
~= P('hollywood fashion website' | read) * P(read) 
~= P('hollywood' | read) * P('fashion' | read) * P('website' | read) * P(read)
~= 0.25/3 * 0.25/3 * 1/3 * P(read)
~= 0.0023 * 3/6
~= 0.001

P(ignore | 'hollywood fashion website')
~= P('hollywood fashion website' | ignore) * P(ignore) 
~= P('hollywood' | ignore) * P('fashion' | ignore) * P('website' | ignore) * P(ignore)
~= 2.75/3 * 0.75/3 * 1/3 * P(ignore)
~= 0.076 * 3/6
~= 0.038

P(read | 'hollywood fashion website')   ~= 0.001 ~= 0.03
P(ignore | 'hollywood fashion website') ~= 0.038 ~= 0.97
</pre>
</p>

<p>
which gives us new values for the docs 5 and 6
<table>
<thead><tr><td>feed</td>         <td>doc</td> <td>article text</td>                <td>P(read)</td> <td>P(ignore)</td></tr></thead>
<tbody>
<tr><td>the register</td> <td>1</td> <td>new linux goes fast</td>           <td>1.00</td> <td>0.00</td></tr>
<tr><td>perez hilton</td> <td>2</td> <td>hollywood celebrity goes fast</td> <td>0.00</td> <td>1.00</td></tr>
<tr><td>the register</td> <td>3</td> <td>quake runs on linux</td>           <td>1.00</td> <td>0.00</td></tr>
<tr><td>perez hilton</td> <td>4</td> <td>quake occurs in hollywood</td>     <td>0.00</td> <td>1.00</td></tr>
<tr><td>the register</td> <td>5</td> <td>website on linux</td>              <td>0.92</td> <td>0.08</td></tr>
<tr><td>perez hilton</td> <td>6</td> <td>website on hollywood fashion</td>  <td>0.03</td> <td>0.97</td></tr>
</tbody>
</table>
</p>

<p>
this process of retraining and reclassifying the initially unlablled articles can be reiterated until the probability
distrubtion converges. (note: in this case it looks like it's moving towards a clear case of to-read or to-ignore but it's not
always like that)
</p>

<p>
once the probabilities have converged we can classify our new document 'movie on fashion' using the classifier training with all 6 examples.
<pre>
P(read | 'movie on fashion')
~= P('movie on fashion' | read) * P(read) 
~= P('movie' | read) * P('on' | read) * P('fashion' | read) * P(read)  
~= 0/2.95 * 1/2.95 * 0.03/2.95 * P(read)
~= 1/5.95 * 2/5.95 * 1.03/5.95 * P(read)
~= 2.03/210 * 2.95/6
~= 0.0047

P(ignore | 'movie on fashion')
~= P('movie on fashion' | ignore) * P(ignore) 
~= P('movie' | ignore) * P('on' | ignore) * P('fashion' | ignore) * P(ignore)  
~= 0/3.05 * 0/3.05 * 0.97/3.05 * P(ignore)
~= 1/6.05 * 1/6.05 * 1.97/6.05 * P(ignore)
~= 1.97/221.4 * 3.05/6
~= 0.0045

P(read | 'movie on fashion')   ~= 0.0047 ~= 0.51
P(ignore | 'movie on fashion') ~= 0.0045 ~= 0.49
</pre>
</p>

<p>
recall that previously this was being classified by the normal naive bayes classifier as P(read) ~= 0.66 and P(ignore) ~= 0.33
</p>

<p>
we can see now how the inclusion of 'fashion', as related to 'hollywood' through doc 6, has increased the to-ignore probability of this example.
</p>

<p>
so this semi supervised algorithm looks promising on paper but the real question is <a href="does_it_do_any_better.html">does it do any better</a> on real data?
</p>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-10409220-1");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
                                                        
</html>
